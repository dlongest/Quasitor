{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import feedparser\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sec_feed():\n",
    "    d = feedparser.parse('https://www.sec.gov/rss/litigation/litreleases.xml')\n",
    "    \n",
    "    return [LitigationRelease(entry) for entry in d.entries]\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitigationRelease(object):\n",
    "    \n",
    "    def __init__(self, entry_dict):\n",
    "        # Everything in this top section is available in the entry_dict argument meaning\n",
    "        # it could be taken from the RSS feed directly\n",
    "        self.id = entry_dict['id']\n",
    "        self.title = entry_dict['title']\n",
    "        self.url = self._validate_url(entry_dict['link'])\n",
    "        self.publish_date = self._parse_publish_date(entry_dict['published'][:-4]) # chop the 3 char time zone \n",
    "        \n",
    "        # Everything below here requires calling read_url to load from the actual litigation release \n",
    "        # as opposed to being available in the original dictionary\n",
    "        self.html = None\n",
    "        self.paragraphs = None\n",
    "        self.entities = None\n",
    "        self.entity_paragraph_slice = slice(None, None) # default to using all paragraphs\n",
    "    \n",
    "    def _parse_publish_date(self, date_string):\n",
    "        \"\"\" Given a date string pulled from the SEC RSS feed, parses it into a datetime object.  In most cases\n",
    "        the date string will be in the format \"DayOfWeek, Day Month Year Hour:Minute:Second.  However, in some cases\n",
    "        the seconds are omitted.  It's necessary to use two different strptime format strings to parse it, which \n",
    "        is accounted for in this method.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return datetime.strptime(date_string, '%a, %d %b %Y %H:%M:%S')\n",
    "        except:\n",
    "            return datetime.strptime(date_string, '%a, %d %b %Y %H:%M')\n",
    "    \n",
    "    def _validate_url(self, url):\n",
    "        if self.id == 'LR-23731': # the SEC feed for 23731 has a bad URL.  This is a hack to fix it like this.\n",
    "            return 'https://www.sec.gov/litigation/litreleases/2017/lr23731.htm'\n",
    "        else:\n",
    "            return url\n",
    "    \n",
    "    def read_url(self):       \n",
    "        \n",
    "        try_count = 0\n",
    "        \n",
    "        while try_count < 3:\n",
    "        \n",
    "            try:\n",
    "                self._try_read_url()\n",
    "                return\n",
    "            except Exception as inst:\n",
    "                print('Error on ID {} @ {}.\\n{}\\n Retry # {}'.format(self.id, self.url, inst, try_count + 1))\n",
    "                try_count += 1\n",
    "                \n",
    "    def _try_read_url(self):\n",
    "        \"\"\" Opens the URl at self.url which is treated as an SEC Litigation Release.  The body is parsed\n",
    "        to locate all paragraphs containing potential entities.  The format changed at the start of 2018\n",
    "        so that we use 2 different approaches.  For 2018 releases, we look for an ID and then ignore\n",
    "        the last paragraph (which is SEC internal attribution).  For 2017 releases there is no such \n",
    "        ID so we have to just take all paragraphs.  We also have to ignore the last 3 paragraphs since\n",
    "        there are 2 additional ones from the 2018 version.  \n",
    "        \"\"\"\n",
    "        with urllib.request.urlopen(self.url) as response:\n",
    "            self.html = response.read()\n",
    "\n",
    "            soup = BeautifulSoup(self.html, 'html.parser')\n",
    "\n",
    "            body = soup.find(id='main-content')\n",
    "\n",
    "            if body is not None:        \n",
    "                self.paragraphs = [unicodedata.normalize('NFKD', p.text) for p in body.find_all('p')]     \n",
    "                self.entity_paragraph_slice = slice(None, -1)\n",
    "            else:            \n",
    "                self.paragraphs = [unicodedata.normalize('NFKD', p.text) for p in soup.find_all('p')]  \n",
    "                self.entity_paragraph_slice = slice(None, -4)\n",
    "    \n",
    "    def parse_entities(self, parser):\n",
    "        \"\"\" Assuming read_url() has already been called, parse_entities will iterate through self.paragraphs\n",
    "        calling parser on each one.  Parser must be able to convert each paragraph (a string) to a collection\n",
    "        of tuples in the form (ENTITY, LABEL).  The tuples are added to self.entities, which is a set to prevent\n",
    "        duplication.  If read_url() has not been called (based on self.paragraphs being None), it returns\n",
    "        without error and without altering self.entities.        \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.paragraphs is None:\n",
    "            return\n",
    "        \n",
    "        self.entities = set([])\n",
    "       \n",
    "        for paragraph in self.paragraphs[self.entity_paragraph_slice]:\n",
    "            \n",
    "            new_entities = parser(paragraph)\n",
    "            \n",
    "            for ent in new_entities:            \n",
    "                self.entities.add(ent)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return ' - '.join([self.id, self.title])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityParser(object):\n",
    "    \n",
    "    def __init__(self, nlp, included_types = None):\n",
    "        self.nlp = nlp\n",
    "        \n",
    "        if included_types is not None:\n",
    "            self.included_types = included_types\n",
    "        else:\n",
    "            self.included_types = ['ORG', 'PERSON']\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        \n",
    "        entities = [(ent.text, ent.label_) for ent in self.nlp(text).ents]\n",
    "        \n",
    "        return self._filter_entities(entities)\n",
    "    \n",
    "    def _filter_entities(self, entities):\n",
    "        return [e for e in entities if e[1] in self.included_types]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_release(release, entity_parser):\n",
    "    \n",
    "    release.read_url()\n",
    "    \n",
    "    release.parse_entities(entity_parser)\n",
    "    \n",
    "\n",
    "def save_release_entities(releases, csv_filename):\n",
    "    \n",
    "    import csv\n",
    "   \n",
    "    def write_release_entities(writer, release):\n",
    "        row = [release.id, str(release.publish_date), release.title, ] + \\\n",
    "                ['{} ({})'.format(name, label) for (name, label) in release.entities]\n",
    "        \n",
    "        encoded_row = [r.encode('ascii', 'ignore').decode('ascii') for r in row]\n",
    "        \n",
    "        writer.writerow(encoded_row)\n",
    "    \n",
    "    with open(csv_filename, 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter = '|')\n",
    "        \n",
    "        for release in releases:\n",
    "            write_release_entities(writer, release)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go():\n",
    "    parser = EntityParser(nlp)\n",
    "\n",
    "    releases = read_sec_feed()\n",
    "\n",
    "    for release in releases:\n",
    "        process_release(release, parser) \n",
    "\n",
    "    save_release_entities(releases, 'sec-feed-entities.csv')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
